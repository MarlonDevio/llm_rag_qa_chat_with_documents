{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-06T09:01:27.851319Z",
     "start_time": "2024-05-06T09:01:27.544365Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "model_names = {\n",
    "\t\"gpt-3.5\": \"gpt-3.5-turbo-0125\",\n",
    "\t\"gpt-4\": \"gpt-4-turbo\",\n",
    "\t\"opus\": \"claude-3-opus-20240229\",\n",
    "\t\"sonnet\": \"claude-3-sonnet-2024022\",\n",
    "\t\"haiku\": \"claude-3-haiku-20240307\"}\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:01:31.026295Z",
     "start_time": "2024-05-06T09:01:31.023363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ask_gpt(question, model=model_names.get(\"gpt-3.5\")):\n",
    "\treturn llm.invoke(question, model=model)\n",
    "\n",
    "\n",
    "def ask_gpt_3(question):\n",
    "\treturn ask_gpt(question).content\n",
    "\n",
    "\n",
    "def ask_gpt_4(question):\n",
    "\treturn ask_gpt(question, model=model_names.get(\"gpt-4\")).content"
   ],
   "id": "f959189e6f0d6d95",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Anthropic Model",
   "id": "99cd31fabaa9725e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:05:06.581823Z",
     "start_time": "2024-05-06T09:05:06.579206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ask_claude(question, model):\n",
    "\tchat_model = ChatAnthropic(model=model)\n",
    "\treturn chat_model.invoke(question).content\n",
    "\n",
    "\n",
    "def ask_opus(question):\n",
    "\treturn ask_claude(question, model_names.get(\"opus\"))\n",
    "\n",
    "\n",
    "def ask_sonnet(question):\n",
    "\treturn ask_claude(question, model_names.get(\"sonnet\"))\n",
    "\n",
    "\n",
    "def ask_haiku(question):\n",
    "\treturn ask_claude(question, model_names.get(\"haiku\"))"
   ],
   "id": "e2a90be42b178d00",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.schema import (\n",
    "\tSystemMessage,\n",
    "\tAIMessage,\n",
    "\tHumanMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "\tSystemMessage(content=\"You are an expert in writing dutch poems\"),\n",
    "\tHumanMessage(content=\"write a short poem in dutch and end with the name of your Creator. OpenAi or Anthropic\")\n",
    "]"
   ],
   "id": "d7091a25d0f6b52e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Caching LLM Responses\n",
    "\n",
    "## in memory Cache"
   ],
   "id": "53049adabbecdcd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache"
   ],
   "id": "8dc67f45e80a302d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# set_llm_cache(InMemoryCache())\n",
    "# ask_gpt_3(\"What is your funniest joke?\")"
   ],
   "id": "7b70b74160be72ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "ask_opus(\"Who made you?\")"
   ],
   "id": "763ecd70996beb58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time \n",
    "ask_gpt_4(\"who are you? what model are you based on? Chatgpt 3 or chatgpt 4\")"
   ],
   "id": "47c81b64054e0e2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SQLite Caching",
   "id": "561c8760eebfa619"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n",
    "\n",
    "# first request not in cache takes longer\n",
    "ask_gpt_3(\"Tell me a joke\")\n",
    "\n",
    "# second (cached, faster)\n",
    "ask_gpt_3(\"Tell me a joke\")"
   ],
   "id": "bd1bd052081b6a2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LLM Streaming",
   "id": "2ca729b46868c0de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "llm = ChatOpenAI()\n",
    "for chunk in llm.stream(\"What gpt model are you? 3 or 4 and write a poem about that\", model=\"gpt-4-turbo\"):\n",
    "\tprint(chunk.content, end=\"\", flush=True)\n",
    "\n"
   ],
   "id": "4f738e1bf0ef2c04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PromptTemplates",
   "id": "6c903caa78c7494b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = '''You are an expert in building a RAG machine learning model and running it locally with Langchain and HuggingFace models. The models used will be {model} and the programming language used will be {language}. We will run it on the OS {OS}.'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "prompt = prompt_template.format(model=\"Llama2\", language=\"Python\", OS=\"MacOS\")\n",
    "\n",
    "prompt"
   ],
   "id": "5ef2234ea5a2d2ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ask_opus(prompt)",
   "id": "19bea1d1c4135edf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ask_gpt_4(prompt)",
   "id": "b8d3cb4890dbada5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ChatPromptTemplates ",
   "id": "7b3ec773774c4fc3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T08:59:45.985759Z",
     "start_time": "2024-05-06T08:59:45.145011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "\t[\n",
    "\t\tSystemMessage(content='You respond only in JSON format.'),\n",
    "\t\tHumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population')\n",
    "\t]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(n='10', area='Europe')\n",
    "\n",
    "print(messages)"
   ],
   "id": "e1fa08e9898971ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You respond only in JSON format.'), HumanMessage(content='Top 10 countries in Europe by population')]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:02:09.957433Z",
     "start_time": "2024-05-06T09:02:04.949493Z"
    }
   },
   "cell_type": "code",
   "source": "print(ask_gpt_3(messages))",
   "id": "6f30bb7f931f59e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"1\": {\n",
      "        \"country\": \"Germany\",\n",
      "        \"population\": 83783942\n",
      "    },\n",
      "    \"2\": {\n",
      "        \"country\": \"France\",\n",
      "        \"population\": 65273511\n",
      "    },\n",
      "    \"3\": {\n",
      "        \"country\": \"United Kingdom\",\n",
      "        \"population\": 67886011\n",
      "    },\n",
      "    \"4\": {\n",
      "        \"country\": \"Italy\",\n",
      "        \"population\": 60461826\n",
      "    },\n",
      "    \"5\": {\n",
      "        \"country\": \"Spain\",\n",
      "        \"population\": 46754778\n",
      "    },\n",
      "    \"6\": {\n",
      "        \"country\": \"Ukraine\",\n",
      "        \"population\": 43733762\n",
      "    },\n",
      "    \"7\": {\n",
      "        \"country\": \"Poland\",\n",
      "        \"population\": 37887768\n",
      "    },\n",
      "    \"8\": {\n",
      "        \"country\": \"Romania\",\n",
      "        \"population\": 19237691\n",
      "    },\n",
      "    \"9\": {\n",
      "        \"country\": \"Netherlands\",\n",
      "        \"population\": 17134872\n",
      "    },\n",
      "    \"10\": {\n",
      "        \"country\": \"Belgium\",\n",
      "        \"population\": 11589623\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:04:08.735430Z",
     "start_time": "2024-05-06T09:04:05.921061Z"
    }
   },
   "cell_type": "code",
   "source": "print(ask_haiku(messages))",
   "id": "f0b216e0f36ddb14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the top 10 countries in Europe by population:\n",
      "\n",
      "{\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"rank\": 1,\n",
      "      \"country\": \"Russia\",\n",
      "      \"population\": 146,748,590\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 2,\n",
      "      \"country\": \"Germany\",\n",
      "      \"population\": 83,166,711\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 3,\n",
      "      \"country\": \"United Kingdom\",\n",
      "      \"population\": 67,886,011\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 4,\n",
      "      \"country\": \"France\",\n",
      "      \"population\": 67,391,582\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 5,\n",
      "      \"country\": \"Italy\",\n",
      "      \"population\": 60,367,477\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 6,\n",
      "      \"country\": \"Spain\",\n",
      "      \"population\": 46,754,778\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 7,\n",
      "      \"country\": \"Poland\",\n",
      "      \"population\": 37,846,611\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 8,\n",
      "      \"country\": \"Romania\",\n",
      "      \"population\": 19,237,691\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 9,\n",
      "      \"country\": \"Netherlands\",\n",
      "      \"population\": 17,134,872\n",
      "    },\n",
      "    {\n",
      "      \"rank\": 10,\n",
      "      \"country\": \"Belgium\",\n",
      "      \"population\": 11,589,623\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:05:55.572146Z",
     "start_time": "2024-05-06T09:05:43.010974Z"
    }
   },
   "cell_type": "code",
   "source": "print(ask_opus(messages))",
   "id": "89b7c43a272eca40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the top 10 countries in Europe by population, in JSON format:\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"rank\": 1,\n",
      "    \"country\": \"Russia\",\n",
      "    \"population\": 145934462\n",
      "  },\n",
      "  {\n",
      "    \"rank\": 2,\n",
      "    \"country\": \"Germany\",\n",
      "    \"population\": 83783942\n",
      "  },\n",
      "  {\n",
      "    \"rank\": 3,\n",
      "    \"country\": \"United Kingdom\",\n",
      "    \"population\": 67886011\n",
      "  },\n",
      "  {\n",
      "    \"rank\": 4,\n",
      "    \"country\": \"France\",\n",
      "    \"population\": 65273511\n",
      "  },\n",
      "  {\n",
      "    \"rank\": 5,\n",
      "    \"country\": \"Italy\",\n",
      "    \"population\": 60461826\n",
      "  },\n",
      "  {\n",
      "    \"rank\": 6,\n",
      "    \"country\": \"Spain\",\n",
      "    \"population\": 46754778\n",
      "  },\n",
      "  {\n",
      "    \"rank\": 7,\n",
      "    \"country\": \"Ukraine\",\n",
      "    \"population\": 43733762\n",
      "  },\n",
      "  {\n",
      "    \"rank\": 8,\n",
      "    \"country\": \"Poland\",\n",
      "    \"population\": 37846611\n",
      "  },\n",
      "  {\n",
      "    \"rank\": 9,\n",
      "    \"country\": \"Romania\",\n",
      "    \"population\": 19237691\n",
      "  },\n",
      "  {\n",
      "    \"rank\": 10,\n",
      "    \"country\": \"Netherlands\",\n",
      "    \"population\": 17134872\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Simple Chains",
   "id": "d344c539bc81466f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:18:59.114376Z",
     "start_time": "2024-05-06T09:18:56.856473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(model=model_names.get(\"gpt-3.5\"))\n",
    "\n",
    "template = '''You are an expert in planning everyday tasks. I want you to order the following tasks: {tasks} by the following criteria: {criteria}'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "# prompt = prompt_template.format(tasks=\"cleaning the house, studying for school, buying a gift for my wife\",\n",
    "#                                 criteria=\"importance\")\n",
    "\n",
    "chain = LLMChain(\n",
    "\tllm=llm,\n",
    "\tprompt=prompt_template,\n",
    "\tverbose=True\n",
    ")\n",
    "\n",
    "output = chain.invoke(\n",
    "\t{'tasks': 'studying for school, buying a gift for my wife\"s birthday which is today, cleaning the house',\n",
    "\t 'criteria': 'importance'}\n",
    ")"
   ],
   "id": "c5288035c9d1be08",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:19:18.879410Z",
     "start_time": "2024-05-06T09:19:18.877610Z"
    }
   },
   "cell_type": "code",
   "source": "print(output)",
   "id": "a5b2226a93b28c17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tasks': 'studying for school, buying a gift for my wife\"s birthday which is today, cleaning the house', 'criteria': 'importance', 'text': \"1. Buying a gift for your wife's birthday (since it is today and time-sensitive)\\n2. Studying for school (to ensure you stay on top of your academic responsibilities)\\n3. Cleaning the house (while important, can potentially be done at a later time if necessary)\"}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:24:05.946069Z",
     "start_time": "2024-05-06T09:23:48.490199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tasks = input('Enter tasks: ')\n",
    "criteria = input('Enter criteria: ')\n"
   ],
   "id": "e729f489c2f636ff",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T09:24:59.977442Z",
     "start_time": "2024-05-06T09:24:59.150652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output=chain.invoke({'tasks': tasks, 'criteria': criteria})\n",
    "\n",
    "print(output['text'])"
   ],
   "id": "65a8f2fcee6dca35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Sleep\n",
      "2. Cleaning the house\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sequential chains\n",
    "\n",
    "Make a series of calls to one or more LLM's. You can take the output from one chain and use it as the input to another chain.\n",
    "\n",
    "## 2 types:\n",
    "1. SimpleSequentialChain\n",
    "2. General form of sequential chains\n",
    "\n",
    "### SimpleSequentialChain\n",
    "- Series of chains, each individual chain has a single input and a single output. \n",
    "- Output is used as input to the next step\n",
    "\n",
    "### General Form of sequential chains\n"
   ],
   "id": "a490d1afd140e9ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "# Experienced python programmer\n",
    "gpt_3_5 = ChatOpenAI(model=model_names.get('gpt-3.5'), temperature=1.2)\n",
    "prompt_template_programmer = 'You are an expert in Python programming. Write a function that implements the concept of {concept}'\n",
    "prompt_programmer = PromptTemplate.from_template(\n",
    "\ttemplate=prompt_template_programmer\n",
    ")\n",
    "\n",
    "opus = ChatAnthropic(model=model_names.get('opus'))\n",
    "\n"
   ],
   "id": "9b2d25af44c7ac0f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
