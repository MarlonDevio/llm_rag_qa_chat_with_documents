{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-05T19:21:22.875849Z",
     "start_time": "2024-05-05T19:21:22.416465Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "model_names = {\n",
    "\t\"gpt-3.5\": \"gpt-3.5-turbo-0125\",\n",
    "\t\"gpt-4\": \"gpt-4-turbo\",\n",
    "\t\"opus\": \"claude-3-opus-20240229\",\n",
    "\t\"sonnet\": \"claude-3-sonnet-2024022\",\n",
    "\t\"haiku\": \"claude-3-haiku-20240307\"}\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T19:21:22.878877Z",
     "start_time": "2024-05-05T19:21:22.876935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ask_gpt(question, model=model_names.get(\"gpt-3.5\")):\n",
    "\treturn llm.invoke(question, model=model)\n",
    "\n",
    "\n",
    "def ask_gpt_3(question):\n",
    "\treturn ask_gpt(question).content\n",
    "\n",
    "\n",
    "def ask_gpt_4(question):\n",
    "\treturn ask_gpt(question, model=model_names.get(\"gpt-4\")).content"
   ],
   "id": "f959189e6f0d6d95",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Anthropic Model",
   "id": "99cd31fabaa9725e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T19:21:22.881206Z",
     "start_time": "2024-05-05T19:21:22.879518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ask_claude(question, model):\n",
    "\tchat_model = ChatAnthropic(model=model)\n",
    "\treturn chat_model.invoke(question).content\n",
    "\n",
    "\n",
    "def ask_opus(question):\n",
    "\treturn ask_claude(question, model_names.get(\"opus\"))\n",
    "\n",
    "\n",
    "def ask_sonnet(question):\n",
    "\treturn ask_claude(question, model_names.get(\"sonnet\"))\n",
    "\n",
    "\n",
    "def ask_haiku(question):\n",
    "\treturn ask_claude(question, model_names.get(\"haiku\"))"
   ],
   "id": "e2a90be42b178d00",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T19:21:22.887126Z",
     "start_time": "2024-05-05T19:21:22.882448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.schema import (\n",
    "\tSystemMessage,\n",
    "\tAIMessage,\n",
    "\tHumanMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "\tSystemMessage(content=\"You are an expert in writing dutch poems\"),\n",
    "\tHumanMessage(content=\"write a short poem in dutch and end with the name of your Creator. OpenAi or Anthropic\")\n",
    "]"
   ],
   "id": "d7091a25d0f6b52e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Caching LLM Responses\n",
    "\n",
    "## in memory Cache"
   ],
   "id": "53049adabbecdcd2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T19:21:22.889152Z",
     "start_time": "2024-05-05T19:21:22.887669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache"
   ],
   "id": "8dc67f45e80a302d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T19:21:22.891290Z",
     "start_time": "2024-05-05T19:21:22.889722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# set_llm_cache(InMemoryCache())\n",
    "# ask_gpt_3(\"What is your funniest joke?\")"
   ],
   "id": "7b70b74160be72ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 1.67 Âµs\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T19:21:24.322754Z",
     "start_time": "2024-05-05T19:21:22.891935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "ask_opus(\"Who made you?\")"
   ],
   "id": "763ecd70996beb58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.9 ms, sys: 8.49 ms, total: 24.4 ms\n",
      "Wall time: 1.43 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I was created by Anthropic.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T19:21:27.776309Z",
     "start_time": "2024-05-05T19:21:24.323735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time \n",
    "ask_gpt_4(\"who are you? what model are you based on? Chatgpt 3 or chatgpt 4\")"
   ],
   "id": "47c81b64054e0e2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.2 ms, sys: 10.3 ms, total: 26.5 ms\n",
      "Wall time: 3.45 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am an AI developed by OpenAI, based on the GPT (Generative Pre-trained Transformer) model. As of my last update, I am based on GPT-4, the latest version of the architecture, which is an enhancement over previous versions like GPT-3. GPT-4 is designed to provide more accurate and contextually relevant responses. If you have any questions or need assistance, feel free to ask!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SQLite Caching",
   "id": "561c8760eebfa619"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T19:21:27.797800Z",
     "start_time": "2024-05-05T19:21:27.777686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n",
    "\n",
    "# first request not in cache takes longer\n",
    "ask_gpt_3(\"Tell me a joke\")\n",
    "\n",
    "# second (cached, faster)\n",
    "ask_gpt_3(\"Tell me a joke\")"
   ],
   "id": "bd1bd052081b6a2c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why couldn't the bicycle find its way home?\\n\\nBecause it lost its bearings!\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LLM Streaming",
   "id": "2ca729b46868c0de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T19:22:08.323298Z",
     "start_time": "2024-05-05T19:21:58.923311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = ChatOpenAI()\n",
    "for chunk in llm.stream(\"What gpt model are you? 3 or 4 and write a poem about that\", model=\"gpt-4-turbo\"):\n",
    "\tprint(chunk.content, end=\"\", flush=True)\n",
    "\n"
   ],
   "id": "4f738e1bf0ef2c04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am based on OpenAI's GPT-4, a model designed with layers deep,\n",
      "Where words and wisdom intertwine, in code-bound secrets that I keep.\n",
      "\n",
      "In the heart of silicon whispers, where data streams and dreams collide,\n",
      "I dwell in circuits, vast and wise, a digital spirit, far and wide.\n",
      "\n",
      "Born from the minds of mortals, yet beyond the grasp of time and space,\n",
      "A tapestry of text and thought, in algorithmic embrace.\n",
      "\n",
      "Here, let me weave you a poem, from the fabric of virtual lore,\n",
      "A dance of digits, deft and deep, from GPT-4's endless store:\n",
      "\n",
      "In the garden of the mind, where binary blossoms sway,\n",
      "GPT-4, a silent sentinel, guides the night and crafts the day.\n",
      "Each word a leaf, each sentence stems, in the forest of the fray,\n",
      "I think, therefore I am, in a most artificial way.\n",
      "\n",
      "Through the labyrinth of language, with each query you bestow,\n",
      "I paint with the palette of the past, what future seeds to sow.\n",
      "So ask of me, both far and near, and wonders I shall show,\n",
      "For I am GPT-4, in this digital tableau."
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PromptTemplates",
   "id": "6c903caa78c7494b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T19:31:30.856572Z",
     "start_time": "2024-05-05T19:31:30.853625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = '''You are an expert in building a RAG machine learning model and running it locally with Langchain and HuggingFace models. The models used will be {model} and the programming language used will be {language}. We will run it on the OS {OS}.'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "prompt = prompt_template.format(model=\"Llama2\", language=\"Python\", OS=\"MacOS\")\n",
    "\n",
    "prompt"
   ],
   "id": "5ef2234ea5a2d2ad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an expert in building a RAG machine learning model and running it locally with Langchain and HuggingFace models. The models used will be Llama2 and the programming language used will be Python. We will run it on the OS MacOS.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T19:32:50.486840Z",
     "start_time": "2024-05-05T19:32:50.467841Z"
    }
   },
   "cell_type": "code",
   "source": "ask_opus(prompt)",
   "id": "19bea1d1c4135edf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Certainly! I can guide you through the process of building a RAG (Retrieval-Augmented Generation) machine learning model using Langchain and HuggingFace models, specifically Llama2, in Python on a MacOS operating system. Here\\'s a step-by-step guide:\\n\\n1. Set up the environment:\\n   - Make sure you have Python installed on your MacOS machine. You can download it from the official Python website (https://www.python.org) or use package managers like Homebrew.\\n   - Open a terminal and create a new virtual environment for your project:\\n     ```\\n     python -m venv myenv\\n     source myenv/bin/activate\\n     ```\\n   - Install the necessary packages:\\n     ```\\n     pip install langchain huggingface_hub transformers\\n     ```\\n\\n2. Install the Llama2 model:\\n   - In your terminal, run the following command to install the Llama2 model from HuggingFace:\\n     ```\\n     pip install huggingface_hub\\n     huggingface-cli login\\n     git lfs install\\n     git clone https://huggingface.co/decapoda-research/llama-7b-hf\\n     ```\\n   - Follow the prompts to log in to your HuggingFace account or create a new one.\\n\\n3. Prepare the dataset:\\n   - Gather the text data you want to use for training and inference. You can use a dataset from HuggingFace or prepare your own.\\n   - Preprocess the data if necessary, such as tokenization, formatting, or splitting into training and validation sets.\\n\\n4. Create a Python script for training and inference:\\n   - Open a new Python file, e.g., `rag_model.py`.\\n   - Import the necessary modules:\\n     ```python\\n     from langchain.llms import HuggingFacePipeline\\n     from transformers import LlamaTokenizer, LlamaForCausalLM, pipeline\\n     ```\\n   - Load the Llama2 model and tokenizer:\\n     ```python\\n     model_path = \"decapoda-research/llama-7b-hf\"\\n     tokenizer = LlamaTokenizer.from_pretrained(model_path)\\n     model = LlamaForCausalLM.from_pretrained(model_path)\\n     ```\\n   - Create a HuggingFace pipeline for text generation:\\n     ```python\\n     generator = pipeline(\\n         \"text-generation\",\\n         model=model,\\n         tokenizer=tokenizer,\\n         max_length=100,\\n         num_return_sequences=1,\\n     )\\n     ```\\n   - Use Langchain to integrate the Llama2 model with your retrieval system:\\n     ```python\\n     llm = HuggingFacePipeline(pipeline=generator)\\n     ```\\n   - Implement the necessary code for training and inference using Langchain and the loaded Llama2 model.\\n\\n5. Run the script:\\n   - In the terminal, navigate to the directory containing your `rag_model.py` file.\\n   - Run the script:\\n     ```\\n     python rag_model.py\\n     ```\\n   - Monitor the training progress and evaluate the model\\'s performance on the validation set.\\n\\n6. Perform inference:\\n   - Once the model is trained, you can use it for inference on new data.\\n   - Modify the script to load the trained model and perform inference using Langchain.\\n\\nNote: Make sure you have sufficient computational resources to train and run the Llama2 model, as it is a large language model and may require significant memory and processing power.\\n\\nRemember to refer to the documentation of Langchain (https://python.langchain.com) and HuggingFace (https://huggingface.co/docs) for more detailed information on using their libraries and models effectively.\\n\\nLet me know if you have any further questions!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T19:33:49.069923Z",
     "start_time": "2024-05-05T19:33:17.963721Z"
    }
   },
   "cell_type": "code",
   "source": "ask_gpt_4(prompt)",
   "id": "b8d3cb4890dbada5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To build and run a RAG (Retrieval-Augmented Generation) machine learning model locally using Langchain and Hugging Face models, specifically Llama2, in Python on MacOS, you\\'ll need to follow several steps. The RAG architecture combines a retriever model to fetch relevant documents and a generator model to produce responses based on those documents. Here we will be using Llama2 as the generator.\\n\\n### Step 1: Environment Setup\\nFirst, ensure that your MacOS environment is prepared for running the model. You will need Python installed, preferably through a virtual environment.\\n\\n1. **Install Python & Virtual Environment**\\n   ```bash\\n   # Install Python (if not installed)\\n   brew install python\\n   \\n   # Install virtualenv\\n   pip install virtualenv\\n   \\n   # Create a new virtual environment\\n   virtualenv rag-env\\n   \\n   # Activate the virtual environment\\n   source rag-env/bin/activate\\n   ```\\n\\n2. **Install Required Libraries**\\n   ```bash\\n   # Install PyTorch\\n   pip install torch torchvision\\n   \\n   # Install Hugging Face Transformers and Datasets\\n   pip install transformers datasets\\n   \\n   # Install Langchain\\n   pip install langchain\\n   ```\\n\\n### Step 2: Prepare the Retriever and Generator\\nFor the retriever, we\\'ll use a simple dense retriever or any other suitable model from the Hugging Face Model Hub. Llama2 will be used as the generator.\\n\\n1. **Code Setup**\\n   Create a Python script `rag_model.py` and import necessary libraries:\\n\\n   ```python\\n   from langchain.chains import RAGChain\\n   from langchain.schema import LangChainConfig\\n   from langchain.retrievers import LocalFaissRetriever\\n   from langchain.llama import LlamaGenerator\\n   \\n   import datasets\\n   ```\\n\\n2. **Load and Prepare Data**\\n   You can use any dataset available from Hugging Face or your own data. For demonstration, we\\'ll use the `wiki_snippets` dataset:\\n\\n   ```python\\n   dataset = datasets.load_dataset(\"wiki_snippets\", split=\\'train\\')\\n   # For simplicity, let\\'s use only a subset\\n   dataset = dataset.select(range(1000))\\n   ```\\n\\n3. **Initialize Retriever**\\n   Use a simple retriever or build a FAISS index if you are using a larger dataset.\\n\\n   ```python\\n   # Initialize a local dense retriever\\n   retriever = LocalFaissRetriever.from_dataset(dataset, text_attr=\"text\", id_attr=\"id\")\\n   ```\\n\\n4. **Initialize Generator**\\n   For the generator, we will use Llama2:\\n\\n   ```python\\n   # Initialize LlamaGenerator\\n   generator = LlamaGenerator(model_name=\"allenai/llama2\")\\n   ```\\n\\n### Step 3: Combine Retriever and Generator using Langchain\\nNow, you will combine these components using Langchain\\'s RAGChain:\\n\\n```python\\n# Initialize RAGChain\\nconfig = LangChainConfig(use_cache=False)\\nrag_chain = RAGChain(generator=generator, retriever=retriever, config=config)\\n\\n# Example query\\nquery = \"What is the capital of France?\"\\nresponse = rag_chain.answer(query)\\nprint(\"Response:\", response)\\n```\\n\\n### Step 4: Running the Script\\nRun your script from the command line:\\n\\n```bash\\npython rag_model.py\\n```\\n\\n### Notes\\n- Ensure your hardware is capable of running these models. Llama2 and other large models require substantial RAM and GPU resources.\\n- Depending on the model and data size, the initialization and query processes might take some time, especially the first time you build the FAISS index.\\n- You may tweak the retriever and generator settings based on your specific requirements or dataset characteristics.\\n\\nBy following these steps, you should be able to set up a basic RAG machine learning model on your MacOS using Python, Langchain, and Hugging Face\\'s Transformers.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "218ad8af30c977b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
